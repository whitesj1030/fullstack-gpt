{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.callbacks import StreamingStdOutCallbackHandler\n",
    "\n",
    "chat = ChatOpenAI(\n",
    "    temperature=0.1,\n",
    "    streaming=True,\n",
    "    callbacks=[\n",
    "        StreamingStdOutCallbackHandler(),\n",
    "    ],\n",
    ")\n",
    "\n",
    "chef_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"\",\n",
    "        ),\n",
    "        (\"human\", \"I want to cook {cuisine} food.\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "chef_chain = chef_prompt | chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That's great! Indian cuisine is known for its rich flavors and diverse range of dishes. Here's a simple recipe for you to try:\n",
      "\n",
      "Butter Chicken (Murgh Makhani):\n",
      "\n",
      "Ingredients:\n",
      "- 500g boneless chicken, cut into pieces\n",
      "- 2 tablespoons butter\n",
      "- 1 onion, finely chopped\n",
      "- 2 teaspoons ginger-garlic paste\n",
      "- 2 teaspoons red chili powder\n",
      "- 1 teaspoon turmeric powder\n",
      "- 1 teaspoon garam masala\n",
      "- 1 cup tomato puree\n",
      "- 1/2 cup heavy cream\n",
      "- Salt to taste\n",
      "- Fresh coriander leaves for garnish\n",
      "\n",
      "Instructions:\n",
      "1. Heat butter in a pan over medium heat. Add the chopped onions and sauté until golden brown.\n",
      "2. Add the ginger-garlic paste and cook for a minute until the raw smell disappears.\n",
      "3. Add the red chili powder, turmeric powder, and garam masala. Mix well and cook for a minute.\n",
      "4. Add the tomato puree and cook for 5-7 minutes until the oil separates from the mixture.\n",
      "5. Add the chicken pieces and season with salt. Cook for about 10 minutes until the chicken is cooked through.\n",
      "6. Reduce the heat to low and stir in the heavy cream. Simmer for another 5 minutes.\n",
      "7. Garnish with fresh coriander leaves and serve hot with naan bread or steamed rice.\n",
      "\n",
      "Enjoy your homemade Butter Chicken!That's a delicious recipe! To make it vegetarian, we can replace the chicken with a vegetarian alternative. Here's a modified version of the recipe using paneer, a type of Indian cheese:\n",
      "\n",
      "Vegetarian Butter Paneer (Paneer Makhani):\n",
      "\n",
      "Ingredients:\n",
      "- 500g paneer, cut into cubes\n",
      "- 2 tablespoons butter\n",
      "- 1 onion, finely chopped\n",
      "- 2 teaspoons ginger-garlic paste\n",
      "- 2 teaspoons red chili powder\n",
      "- 1 teaspoon turmeric powder\n",
      "- 1 teaspoon garam masala\n",
      "- 1 cup tomato puree\n",
      "- 1/2 cup heavy cream\n",
      "- Salt to taste\n",
      "- Fresh coriander leaves for garnish\n",
      "\n",
      "Instructions:\n",
      "1. Heat butter in a pan over medium heat. Add the chopped onions and sauté until golden brown.\n",
      "2. Add the ginger-garlic paste and cook for a minute until the raw smell disappears.\n",
      "3. Add the red chili powder, turmeric powder, and garam masala. Mix well and cook for a minute.\n",
      "4. Add the tomato puree and cook for 5-7 minutes until the oil separates from the mixture.\n",
      "5. Add the paneer cubes and season with salt. Cook for about 5 minutes until the paneer is heated through.\n",
      "6. Reduce the heat to low and stir in the heavy cream. Simmer for another 5 minutes.\n",
      "7. Garnish with fresh"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32md:\\vscodeworks\\fullstack-gpt\\3.4ChangingChanins.ipynb Cell 2\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/vscodeworks/fullstack-gpt/3.4ChangingChanins.ipynb#W1sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m veg_chain \u001b[39m=\u001b[39m veg_chef_prompt \u001b[39m|\u001b[39m chat\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/vscodeworks/fullstack-gpt/3.4ChangingChanins.ipynb#W1sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m final_chain \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39mrecipe\u001b[39m\u001b[39m\"\u001b[39m: chef_chain} \u001b[39m|\u001b[39m veg_chain\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/vscodeworks/fullstack-gpt/3.4ChangingChanins.ipynb#W1sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m final_chain\u001b[39m.\u001b[39;49minvoke({\u001b[39m\"\u001b[39;49m\u001b[39mcuisine\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39m\"\u001b[39;49m\u001b[39mindian\u001b[39;49m\u001b[39m\"\u001b[39;49m})\n",
      "File \u001b[1;32md:\\vscodeworks\\fullstack-gpt\\env\\Lib\\site-packages\\langchain\\schema\\runnable\\base.py:1213\u001b[0m, in \u001b[0;36mRunnableSequence.invoke\u001b[1;34m(self, input, config)\u001b[0m\n\u001b[0;32m   1211\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1212\u001b[0m     \u001b[39mfor\u001b[39;00m i, step \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msteps):\n\u001b[1;32m-> 1213\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m step\u001b[39m.\u001b[39;49minvoke(\n\u001b[0;32m   1214\u001b[0m             \u001b[39minput\u001b[39;49m,\n\u001b[0;32m   1215\u001b[0m             \u001b[39m# mark each step as a child run\u001b[39;49;00m\n\u001b[0;32m   1216\u001b[0m             patch_config(\n\u001b[0;32m   1217\u001b[0m                 config, callbacks\u001b[39m=\u001b[39;49mrun_manager\u001b[39m.\u001b[39;49mget_child(\u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mseq:step:\u001b[39;49m\u001b[39m{\u001b[39;49;00mi\u001b[39m+\u001b[39;49m\u001b[39m1\u001b[39;49m\u001b[39m}\u001b[39;49;00m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m   1218\u001b[0m             ),\n\u001b[0;32m   1219\u001b[0m         )\n\u001b[0;32m   1220\u001b[0m \u001b[39m# finish the root run\u001b[39;00m\n\u001b[0;32m   1221\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[1;32md:\\vscodeworks\\fullstack-gpt\\env\\Lib\\site-packages\\langchain\\chat_models\\base.py:142\u001b[0m, in \u001b[0;36mBaseChatModel.invoke\u001b[1;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[0;32m    131\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39minvoke\u001b[39m(\n\u001b[0;32m    132\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    133\u001b[0m     \u001b[39minput\u001b[39m: LanguageModelInput,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    137\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any,\n\u001b[0;32m    138\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m BaseMessage:\n\u001b[0;32m    139\u001b[0m     config \u001b[39m=\u001b[39m config \u001b[39mor\u001b[39;00m {}\n\u001b[0;32m    140\u001b[0m     \u001b[39mreturn\u001b[39;00m cast(\n\u001b[0;32m    141\u001b[0m         ChatGeneration,\n\u001b[1;32m--> 142\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgenerate_prompt(\n\u001b[0;32m    143\u001b[0m             [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_convert_input(\u001b[39minput\u001b[39;49m)],\n\u001b[0;32m    144\u001b[0m             stop\u001b[39m=\u001b[39;49mstop,\n\u001b[0;32m    145\u001b[0m             callbacks\u001b[39m=\u001b[39;49mconfig\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mcallbacks\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[0;32m    146\u001b[0m             tags\u001b[39m=\u001b[39;49mconfig\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mtags\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[0;32m    147\u001b[0m             metadata\u001b[39m=\u001b[39;49mconfig\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mmetadata\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[0;32m    148\u001b[0m             run_name\u001b[39m=\u001b[39;49mconfig\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mrun_name\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[0;32m    149\u001b[0m             \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[0;32m    150\u001b[0m         )\u001b[39m.\u001b[39mgenerations[\u001b[39m0\u001b[39m][\u001b[39m0\u001b[39m],\n\u001b[0;32m    151\u001b[0m     )\u001b[39m.\u001b[39mmessage\n",
      "File \u001b[1;32md:\\vscodeworks\\fullstack-gpt\\env\\Lib\\site-packages\\langchain\\chat_models\\base.py:459\u001b[0m, in \u001b[0;36mBaseChatModel.generate_prompt\u001b[1;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m    451\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mgenerate_prompt\u001b[39m(\n\u001b[0;32m    452\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    453\u001b[0m     prompts: List[PromptValue],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    456\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any,\n\u001b[0;32m    457\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m LLMResult:\n\u001b[0;32m    458\u001b[0m     prompt_messages \u001b[39m=\u001b[39m [p\u001b[39m.\u001b[39mto_messages() \u001b[39mfor\u001b[39;00m p \u001b[39min\u001b[39;00m prompts]\n\u001b[1;32m--> 459\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgenerate(prompt_messages, stop\u001b[39m=\u001b[39;49mstop, callbacks\u001b[39m=\u001b[39;49mcallbacks, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32md:\\vscodeworks\\fullstack-gpt\\env\\Lib\\site-packages\\langchain\\chat_models\\base.py:349\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[1;34m(self, messages, stop, callbacks, tags, metadata, run_name, **kwargs)\u001b[0m\n\u001b[0;32m    347\u001b[0m         \u001b[39mif\u001b[39;00m run_managers:\n\u001b[0;32m    348\u001b[0m             run_managers[i]\u001b[39m.\u001b[39mon_llm_error(e)\n\u001b[1;32m--> 349\u001b[0m         \u001b[39mraise\u001b[39;00m e\n\u001b[0;32m    350\u001b[0m flattened_outputs \u001b[39m=\u001b[39m [\n\u001b[0;32m    351\u001b[0m     LLMResult(generations\u001b[39m=\u001b[39m[res\u001b[39m.\u001b[39mgenerations], llm_output\u001b[39m=\u001b[39mres\u001b[39m.\u001b[39mllm_output)\n\u001b[0;32m    352\u001b[0m     \u001b[39mfor\u001b[39;00m res \u001b[39min\u001b[39;00m results\n\u001b[0;32m    353\u001b[0m ]\n\u001b[0;32m    354\u001b[0m llm_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_combine_llm_outputs([res\u001b[39m.\u001b[39mllm_output \u001b[39mfor\u001b[39;00m res \u001b[39min\u001b[39;00m results])\n",
      "File \u001b[1;32md:\\vscodeworks\\fullstack-gpt\\env\\Lib\\site-packages\\langchain\\chat_models\\base.py:339\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[1;34m(self, messages, stop, callbacks, tags, metadata, run_name, **kwargs)\u001b[0m\n\u001b[0;32m    336\u001b[0m \u001b[39mfor\u001b[39;00m i, m \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(messages):\n\u001b[0;32m    337\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    338\u001b[0m         results\u001b[39m.\u001b[39mappend(\n\u001b[1;32m--> 339\u001b[0m             \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_generate_with_cache(\n\u001b[0;32m    340\u001b[0m                 m,\n\u001b[0;32m    341\u001b[0m                 stop\u001b[39m=\u001b[39;49mstop,\n\u001b[0;32m    342\u001b[0m                 run_manager\u001b[39m=\u001b[39;49mrun_managers[i] \u001b[39mif\u001b[39;49;00m run_managers \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[0;32m    343\u001b[0m                 \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[0;32m    344\u001b[0m             )\n\u001b[0;32m    345\u001b[0m         )\n\u001b[0;32m    346\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    347\u001b[0m         \u001b[39mif\u001b[39;00m run_managers:\n",
      "File \u001b[1;32md:\\vscodeworks\\fullstack-gpt\\env\\Lib\\site-packages\\langchain\\chat_models\\base.py:492\u001b[0m, in \u001b[0;36mBaseChatModel._generate_with_cache\u001b[1;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m    488\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    489\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAsked to cache, but no cache found at `langchain.cache`.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    490\u001b[0m     )\n\u001b[0;32m    491\u001b[0m \u001b[39mif\u001b[39;00m new_arg_supported:\n\u001b[1;32m--> 492\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_generate(\n\u001b[0;32m    493\u001b[0m         messages, stop\u001b[39m=\u001b[39;49mstop, run_manager\u001b[39m=\u001b[39;49mrun_manager, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs\n\u001b[0;32m    494\u001b[0m     )\n\u001b[0;32m    495\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    496\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_generate(messages, stop\u001b[39m=\u001b[39mstop, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\vscodeworks\\fullstack-gpt\\env\\Lib\\site-packages\\langchain\\chat_models\\openai.py:409\u001b[0m, in \u001b[0;36mChatOpenAI._generate\u001b[1;34m(self, messages, stop, run_manager, stream, **kwargs)\u001b[0m\n\u001b[0;32m    405\u001b[0m \u001b[39mif\u001b[39;00m should_stream:\n\u001b[0;32m    406\u001b[0m     stream_iter \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stream(\n\u001b[0;32m    407\u001b[0m         messages, stop\u001b[39m=\u001b[39mstop, run_manager\u001b[39m=\u001b[39mrun_manager, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs\n\u001b[0;32m    408\u001b[0m     )\n\u001b[1;32m--> 409\u001b[0m     \u001b[39mreturn\u001b[39;00m _generate_from_stream(stream_iter)\n\u001b[0;32m    410\u001b[0m message_dicts, params \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_create_message_dicts(messages, stop)\n\u001b[0;32m    411\u001b[0m params \u001b[39m=\u001b[39m {\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs}\n",
      "File \u001b[1;32md:\\vscodeworks\\fullstack-gpt\\env\\Lib\\site-packages\\langchain\\chat_models\\base.py:57\u001b[0m, in \u001b[0;36m_generate_from_stream\u001b[1;34m(stream)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_generate_from_stream\u001b[39m(stream: Iterator[ChatGenerationChunk]) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m ChatResult:\n\u001b[0;32m     56\u001b[0m     generation: Optional[ChatGenerationChunk] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m---> 57\u001b[0m     \u001b[39mfor\u001b[39;00m chunk \u001b[39min\u001b[39;00m stream:\n\u001b[0;32m     58\u001b[0m         \u001b[39mif\u001b[39;00m generation \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m     59\u001b[0m             generation \u001b[39m=\u001b[39m chunk\n",
      "File \u001b[1;32md:\\vscodeworks\\fullstack-gpt\\env\\Lib\\site-packages\\langchain\\chat_models\\openai.py:375\u001b[0m, in \u001b[0;36mChatOpenAI._stream\u001b[1;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m    372\u001b[0m params \u001b[39m=\u001b[39m {\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs, \u001b[39m\"\u001b[39m\u001b[39mstream\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mTrue\u001b[39;00m}\n\u001b[0;32m    374\u001b[0m default_chunk_class \u001b[39m=\u001b[39m AIMessageChunk\n\u001b[1;32m--> 375\u001b[0m \u001b[39mfor\u001b[39;00m chunk \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcompletion_with_retry(\n\u001b[0;32m    376\u001b[0m     messages\u001b[39m=\u001b[39mmessage_dicts, run_manager\u001b[39m=\u001b[39mrun_manager, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams\n\u001b[0;32m    377\u001b[0m ):\n\u001b[0;32m    378\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(chunk, \u001b[39mdict\u001b[39m):\n\u001b[0;32m    379\u001b[0m         chunk \u001b[39m=\u001b[39m chunk\u001b[39m.\u001b[39mdict()\n",
      "File \u001b[1;32md:\\vscodeworks\\fullstack-gpt\\env\\Lib\\site-packages\\openai\\api_resources\\abstract\\engine_api_resource.py:166\u001b[0m, in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    163\u001b[0m \u001b[39mif\u001b[39;00m stream:\n\u001b[0;32m    164\u001b[0m     \u001b[39m# must be an iterator\u001b[39;00m\n\u001b[0;32m    165\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(response, OpenAIResponse)\n\u001b[1;32m--> 166\u001b[0m     \u001b[39mreturn\u001b[39;00m (\n\u001b[0;32m    167\u001b[0m         util\u001b[39m.\u001b[39mconvert_to_openai_object(\n\u001b[0;32m    168\u001b[0m             line,\n\u001b[0;32m    169\u001b[0m             api_key,\n\u001b[0;32m    170\u001b[0m             api_version,\n\u001b[0;32m    171\u001b[0m             organization,\n\u001b[0;32m    172\u001b[0m             engine\u001b[39m=\u001b[39mengine,\n\u001b[0;32m    173\u001b[0m             plain_old_data\u001b[39m=\u001b[39m\u001b[39mcls\u001b[39m\u001b[39m.\u001b[39mplain_old_data,\n\u001b[0;32m    174\u001b[0m         )\n\u001b[0;32m    175\u001b[0m         \u001b[39mfor\u001b[39;00m line \u001b[39min\u001b[39;00m response\n\u001b[0;32m    176\u001b[0m     )\n\u001b[0;32m    177\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    178\u001b[0m     obj \u001b[39m=\u001b[39m util\u001b[39m.\u001b[39mconvert_to_openai_object(\n\u001b[0;32m    179\u001b[0m         response,\n\u001b[0;32m    180\u001b[0m         api_key,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    184\u001b[0m         plain_old_data\u001b[39m=\u001b[39m\u001b[39mcls\u001b[39m\u001b[39m.\u001b[39mplain_old_data,\n\u001b[0;32m    185\u001b[0m     )\n",
      "File \u001b[1;32md:\\vscodeworks\\fullstack-gpt\\env\\Lib\\site-packages\\openai\\api_requestor.py:692\u001b[0m, in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    690\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Returns the response(s) and a bool indicating whether it is a stream.\"\"\"\u001b[39;00m\n\u001b[0;32m    691\u001b[0m \u001b[39mif\u001b[39;00m stream \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mtext/event-stream\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m result\u001b[39m.\u001b[39mheaders\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mContent-Type\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m--> 692\u001b[0m     \u001b[39mreturn\u001b[39;00m (\n\u001b[0;32m    693\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_interpret_response_line(\n\u001b[0;32m    694\u001b[0m             line, result\u001b[39m.\u001b[39mstatus_code, result\u001b[39m.\u001b[39mheaders, stream\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m\n\u001b[0;32m    695\u001b[0m         )\n\u001b[0;32m    696\u001b[0m         \u001b[39mfor\u001b[39;00m line \u001b[39min\u001b[39;00m parse_stream(result\u001b[39m.\u001b[39miter_lines())\n\u001b[0;32m    697\u001b[0m     ), \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m    698\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    699\u001b[0m     \u001b[39mreturn\u001b[39;00m (\n\u001b[0;32m    700\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_interpret_response_line(\n\u001b[0;32m    701\u001b[0m             result\u001b[39m.\u001b[39mcontent\u001b[39m.\u001b[39mdecode(\u001b[39m\"\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m\"\u001b[39m),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    706\u001b[0m         \u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m    707\u001b[0m     )\n",
      "File \u001b[1;32md:\\vscodeworks\\fullstack-gpt\\env\\Lib\\site-packages\\openai\\api_requestor.py:115\u001b[0m, in \u001b[0;36mparse_stream\u001b[1;34m(rbody)\u001b[0m\n\u001b[0;32m    114\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mparse_stream\u001b[39m(rbody: Iterator[\u001b[39mbytes\u001b[39m]) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Iterator[\u001b[39mstr\u001b[39m]:\n\u001b[1;32m--> 115\u001b[0m     \u001b[39mfor\u001b[39;00m line \u001b[39min\u001b[39;00m rbody:\n\u001b[0;32m    116\u001b[0m         _line \u001b[39m=\u001b[39m parse_stream_helper(line)\n\u001b[0;32m    117\u001b[0m         \u001b[39mif\u001b[39;00m _line \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[1;32md:\\vscodeworks\\fullstack-gpt\\env\\Lib\\site-packages\\requests\\models.py:865\u001b[0m, in \u001b[0;36mResponse.iter_lines\u001b[1;34m(self, chunk_size, decode_unicode, delimiter)\u001b[0m\n\u001b[0;32m    856\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Iterates over the response data, one line at a time.  When\u001b[39;00m\n\u001b[0;32m    857\u001b[0m \u001b[39mstream=True is set on the request, this avoids reading the\u001b[39;00m\n\u001b[0;32m    858\u001b[0m \u001b[39mcontent at once into memory for large responses.\u001b[39;00m\n\u001b[0;32m    859\u001b[0m \n\u001b[0;32m    860\u001b[0m \u001b[39m.. note:: This method is not reentrant safe.\u001b[39;00m\n\u001b[0;32m    861\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    863\u001b[0m pending \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m--> 865\u001b[0m \u001b[39mfor\u001b[39;00m chunk \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39miter_content(\n\u001b[0;32m    866\u001b[0m     chunk_size\u001b[39m=\u001b[39mchunk_size, decode_unicode\u001b[39m=\u001b[39mdecode_unicode\n\u001b[0;32m    867\u001b[0m ):\n\u001b[0;32m    869\u001b[0m     \u001b[39mif\u001b[39;00m pending \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    870\u001b[0m         chunk \u001b[39m=\u001b[39m pending \u001b[39m+\u001b[39m chunk\n",
      "File \u001b[1;32md:\\vscodeworks\\fullstack-gpt\\env\\Lib\\site-packages\\requests\\models.py:816\u001b[0m, in \u001b[0;36mResponse.iter_content.<locals>.generate\u001b[1;34m()\u001b[0m\n\u001b[0;32m    814\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mraw, \u001b[39m\"\u001b[39m\u001b[39mstream\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m    815\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 816\u001b[0m         \u001b[39myield from\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mraw\u001b[39m.\u001b[39mstream(chunk_size, decode_content\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m    817\u001b[0m     \u001b[39mexcept\u001b[39;00m ProtocolError \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    818\u001b[0m         \u001b[39mraise\u001b[39;00m ChunkedEncodingError(e)\n",
      "File \u001b[1;32md:\\vscodeworks\\fullstack-gpt\\env\\Lib\\site-packages\\urllib3\\response.py:624\u001b[0m, in \u001b[0;36mHTTPResponse.stream\u001b[1;34m(self, amt, decode_content)\u001b[0m\n\u001b[0;32m    608\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    609\u001b[0m \u001b[39mA generator wrapper for the read() method. A call will block until\u001b[39;00m\n\u001b[0;32m    610\u001b[0m \u001b[39m``amt`` bytes have been read from the connection or until the\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    621\u001b[0m \u001b[39m    'content-encoding' header.\u001b[39;00m\n\u001b[0;32m    622\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    623\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchunked \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msupports_chunked_reads():\n\u001b[1;32m--> 624\u001b[0m     \u001b[39mfor\u001b[39;00m line \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mread_chunked(amt, decode_content\u001b[39m=\u001b[39mdecode_content):\n\u001b[0;32m    625\u001b[0m         \u001b[39myield\u001b[39;00m line\n\u001b[0;32m    626\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[1;32md:\\vscodeworks\\fullstack-gpt\\env\\Lib\\site-packages\\urllib3\\response.py:828\u001b[0m, in \u001b[0;36mHTTPResponse.read_chunked\u001b[1;34m(self, amt, decode_content)\u001b[0m\n\u001b[0;32m    825\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[0;32m    827\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m--> 828\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_update_chunk_length()\n\u001b[0;32m    829\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchunk_left \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m    830\u001b[0m         \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[1;32md:\\vscodeworks\\fullstack-gpt\\env\\Lib\\site-packages\\urllib3\\response.py:758\u001b[0m, in \u001b[0;36mHTTPResponse._update_chunk_length\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    756\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchunk_left \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    757\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[1;32m--> 758\u001b[0m line \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fp\u001b[39m.\u001b[39mfp\u001b[39m.\u001b[39mreadline()\n\u001b[0;32m    759\u001b[0m line \u001b[39m=\u001b[39m line\u001b[39m.\u001b[39msplit(\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m;\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m1\u001b[39m)[\u001b[39m0\u001b[39m]\n\u001b[0;32m    760\u001b[0m \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\socket.py:706\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    704\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m    705\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 706\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sock\u001b[39m.\u001b[39;49mrecv_into(b)\n\u001b[0;32m    707\u001b[0m     \u001b[39mexcept\u001b[39;00m timeout:\n\u001b[0;32m    708\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_timeout_occurred \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\ssl.py:1278\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[1;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[0;32m   1274\u001b[0m     \u001b[39mif\u001b[39;00m flags \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m   1275\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m   1276\u001b[0m           \u001b[39m\"\u001b[39m\u001b[39mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m\n\u001b[0;32m   1277\u001b[0m           \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m)\n\u001b[1;32m-> 1278\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mread(nbytes, buffer)\n\u001b[0;32m   1279\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1280\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\ssl.py:1134\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[1;34m(self, len, buffer)\u001b[0m\n\u001b[0;32m   1132\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1133\u001b[0m     \u001b[39mif\u001b[39;00m buffer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m-> 1134\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sslobj\u001b[39m.\u001b[39;49mread(\u001b[39mlen\u001b[39;49m, buffer)\n\u001b[0;32m   1135\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1136\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sslobj\u001b[39m.\u001b[39mread(\u001b[39mlen\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "veg_chef_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a vegetarian chef specialized on making traditional recipies vegetarian. You find alternative ingredients and explain their preparation. You don't radically modify the recipe. If there is no alternative for a food just say you don't know how to replace it.\",\n",
    "        ),\n",
    "        (\"human\", \"{recipe}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "veg_chain = veg_chef_prompt | chat\n",
    "\n",
    "\n",
    "final_chain = {\"recipe\": chef_chain} | veg_chain\n",
    "\n",
    "final_chain.invoke({\"cuisine\": \"indian\"})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
